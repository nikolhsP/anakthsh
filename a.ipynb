{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Εργασία εργαστηρίου \"Ανάκτηση πληροφορίας\" \n",
    "#ΓΟΥΣΗΣ ΣΤΑΜΑΤΗΣ ice19390053 & ΠΑΠΑΜΑΚΑΡΙΟΣ ΝΙΚΟΛΑΟΣ ice19390183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η εργασία έχει ως θέμα τη δημιουργία μηχανής αναζήτησης με σκοπό την κατανόηση των θεμελιωδών εννοιών της ανάκτησης πληροφορίας, της\n",
    "ευρετηρίασης, της κατάταξης και της αναζήτησης πληροφορίας, καθώς και πρακτικές δεξιότητες στην επεξεργασία φυσικής γλώσσας και την εφαρμογή αλγορίθμων αναζήτησης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βήμα 1- Συλλογή δεδομένων:\n",
    " Από το Wikipedia ανακτήθηκαν 27371 άρθρα, τα οποία αποθηκεύτηκαν στο αρχείο \"output.json\". Για λόγους ευκολίας και συντομίας, ο κώδικάς έχει γραφτεί έτσι ώστε να επεξεργάζεται μόνο τα πρώτα 3 άρθρα από το σύνολο δεδομένων, χρησιμοποιώντας τον τελεστή \":\", ο οποίος δημιουργεί slice του πίνακα που περιέχει τα πρώτα 3 αρθρά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βήμα 2- Προεπεξεργασία κειμένου (Text Processing):\n",
    "Ο κώδικας διαβάζει δεδομένα από το output.json που περιέχει ta άρθρα, επεξεργάζεται το κείμενο από κάθε άρθρο (κανονικοποίηση, διαχωρισμός σε λέξεις, αφαίρεση stopwords, stemming/lemmatization) και αποθηκεύει το καθαρισμένο κείμενο στο cleaned_output.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός αριθμός άρθρων προς επεξεργασία: 27371\n",
      "Επεξεργάστηκαν 1/27371 άρθρα...\n",
      "Επεξεργάστηκαν 2/27371 άρθρα...\n",
      "Επεξεργάστηκαν 3/27371 άρθρα...\n",
      "Η προεπεξεργασία ολοκληρώθηκε και τα δεδομένα αποθηκεύτηκαν στο 'cleaned_output.json'\n"
     ]
    }
   ],
   "source": [
    "#Εισαγωγή βιβλιοθηκών\n",
    "#----------------------------------------------------------------\n",
    "#Για να διαβάσουμε και να αποθηκεύσουμε δεδομένα σε μορφή JSON\n",
    "import json\n",
    "#Η βιβλιοθήκη re είναι για την επεξεργασία κανονικών εκφράσεων\n",
    "import re\n",
    "#Η βιβλιοθήκη Natural Language Toolkit (NLTK) χρησιμοποιείται για την επεξεργασία φυσικής γλώσσας\n",
    "import nltk\n",
    "#Διάφορα εργαλεία από αυτή τη βιβλιοθήκη\n",
    "from nltk.corpus import stopwords #Λίστα κοινών λέξεων (όπως \"και\", \"ή\", \"ο\", κ.λπ.) που συνήθως δεν προσφέρουν καμία πληροφορία\n",
    "from nltk.tokenize import word_tokenize #Χρησιμοποιείται για τον διαχωρισμό ενός κειμένου σε λέξεις\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer #απλοποίηση λέξεων σε μια ριζική μορφή ή σε βασική λέξη\n",
    "\n",
    "# Λήψη των απαραίτητων εργαλείων του nltk (αν δεν έχουν ήδη κατεβεί)\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Αρχικοποίηση stop words, stemmer, και lemmatizer\n",
    "stop_words = set(stopwords.words('english')) #Φτιάχνει ένα σύνολο με τις κοινές αγγλικές λέξεις που θα αγνοηθούν στην επεξεργασία\n",
    "stemmer = PorterStemmer() #Δημιουργεί ένα αντικείμενο του PorterStemmer για την αφαίρεση της κατάληξης των λέξεων\n",
    "lemmatizer = WordNetLemmatizer() #Δημιουργεί ένα αντικείμενο του WordNetLemmatizer για την αναγνώριση της βασικής μορφής της λέξης\n",
    "\n",
    "# Άνοιγμα του JSON αρχείου με τα δεδομένα\n",
    "with open(\"output.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Λίστα για αποθήκευση των «καθαρισμένων» άρθρων\n",
    "cleaned_data = []\n",
    "\n",
    "# Συνολικός αριθμός άρθρων\n",
    "total_articles = len(data)\n",
    "print(f\"Συνολικός αριθμός άρθρων προς επεξεργασία: {total_articles}\")\n",
    "\n",
    "# Καθαρισμός και προεπεξεργασία κειμένου για κάθε άρθρο\n",
    "for i, article in enumerate(data[:3]): #Διαβαζει μονο τα πρωτα 3 αρθρα\n",
    "    title = article.get(\"title\", \"\")\n",
    "    text = article.get(\"text\", \"\")\n",
    "\n",
    "    # 1. Κανονικοποίηση (μετατροπή σε πεζά)\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Tokenization (Διαχωρισμός του κειμένου σε λέξεις)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 3. Αφαίρεση σημείων στίξης και stop words\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Επιβεβαίωση ότι έχουν αφαιρεθεί τα stop words\n",
    "    filtered_tokens = [word for word in tokens if word in stop_words]\n",
    "    if filtered_tokens:\n",
    "        print(f\"Προσοχή: Τα παρακάτω stop words δεν αφαιρέθηκαν: {filtered_tokens}\")\n",
    "\n",
    "    # 4. Stemming ή Lemmatization\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # 5. Επανασύνθεση των tokens σε καθαρισμένο κείμενο\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # Προσθήκη στο νέο σύνολο δεδομένων\n",
    "    cleaned_article = {\n",
    "        \"title\": title,\n",
    "        \"cleaned_text\": cleaned_text\n",
    "    }\n",
    "    cleaned_data.append(cleaned_article)\n",
    "\n",
    "    # Εκτύπωση προόδου κάθε 1 άρθρο\n",
    "    if (i + 1) % 1 == 0:\n",
    "        print(f\"Επεξεργάστηκαν {i + 1}/{total_articles} άρθρα...\")\n",
    "\n",
    "# Αποθήκευση του καθαρισμένου συνόλου δεδομένων σε νέο JSON αρχείο\n",
    "output_path = \"cleaned_output.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Η προεπεξεργασία ολοκληρώθηκε και τα δεδομένα αποθηκεύτηκαν στο '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βήμα 3. Ευρετήριο (Indexing):\n",
    "Δημιουργία ανεστραμμένης δομής δεδομένων ευρετηρίου (inverted index) για την\n",
    "αποτελεσματική αντιστοίχιση όρων στα έγγραφα στα οποία εμφανίζονται και εφαρμογή δομής δεδομένων για την αποθήκευση του ευρετηρίου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιουργία ευρετηρίου (Inverted Index)\n",
    "inverted_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Αναγνώριση των λέξεων (tokens)\n",
    "    for word in set(lemmatized_tokens):  # Χρησιμοποιούμε set για να μην προσθέσουμε την ίδια λέξη πολλές φορές\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(i)  # Αποθηκεύουμε τον αριθμό του άρθρου (index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για κάθε λέξη χωρίς σημεία στήξης, ελέγχεται αν υπάρχει στο ευρετήριο, αλλιώς δημιουργείται νέα είσοδος. Έπειτα, καταχωρείται ένας δείκτης για τον αριθμό του άρθρου που βρίσκεται η λέξη."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για παράδειγμα, στα 3 άρθρα που επεξεργάστηκαν, η λέξη \"redirect\" εμφανίζεται στα άρθρα με index 0 και 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"redirect\": [\n",
    "    0,\n",
    "    2\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step_2.py --> step3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Λήψη των απαραίτητων εργαλείων του nltk (αν δεν έχουν ήδη κατεβεί)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Αρχικοποίηση stop words, stemmer, και lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Άνοιγμα του JSON αρχείου με τα δεδομένα\n",
    "with open(\"./output.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Λίστα για αποθήκευση των «καθαρισμένων» άρθρων\n",
    "cleaned_data = []\n",
    "\n",
    "# Δημιουργία ευρετηρίου (Inverted Index)\n",
    "inverted_index = {}\n",
    "\n",
    "# Συνολικός αριθμός άρθρων\n",
    "total_articles = len(data)\n",
    "print(f\"Συνολικός αριθμός άρθρων προς επεξεργασία: {total_articles}\")\n",
    "\n",
    "# Καθαρισμός και προεπεξεργασία κειμένου για κάθε άρθρο\n",
    "for i, article in enumerate(data[:3]): #Διαβαζει μονο τα πρωτα 3 αρθρα\n",
    "    title = article.get(\"title\", \"\")\n",
    "    text = article.get(\"text\", \"\")\n",
    "\n",
    "    # 1. Κανονικοποίηση (μετατροπή σε πεζά)\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Tokenization (Διαχωρισμός του κειμένου σε λέξεις)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 3. Αφαίρεση σημείων στίξης και stop words\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Επιβεβαίωση ότι έχουν αφαιρεθεί τα stop words\n",
    "    filtered_tokens = [word for word in tokens if word in stop_words]\n",
    "    if filtered_tokens:\n",
    "        print(f\"Προσοχή: Τα παρακάτω stop words δεν αφαιρέθηκαν: {filtered_tokens}\")\n",
    "\n",
    "    # 4. Stemming ή Lemmatization\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # 5. Επανασύνθεση των tokens σε καθαρισμένο κείμενο\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # Προσθήκη στο νέο σύνολο δεδομένων\n",
    "    cleaned_article = {\n",
    "        \"title\": title,\n",
    "        \"cleaned_text\": cleaned_text\n",
    "    }\n",
    "    cleaned_data.append(cleaned_article)\n",
    "\n",
    "    # Αναγνώριση των λέξεων (tokens)\n",
    "    for word in set(lemmatized_tokens):  # Χρησιμοποιούμε set για να μην προσθέσουμε την ίδια λέξη πολλές φορές\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(i)  # Αποθηκεύουμε τον αριθμό του άρθρου (index)\n",
    "\n",
    "    # Εκτύπωση προόδου κάθε 1 άρθρο\n",
    "    if (i + 1) % 1 == 0:\n",
    "        print(f\"Επεξεργάστηκαν {i + 1}/{total_articles} άρθρα...\")\n",
    "\n",
    "# Αποθήκευση του καθαρισμένου συνόλου δεδομένων σε νέο JSON αρχείο\n",
    "output_path = \"cleaned_output.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Η προεπεξεργασία ολοκληρώθηκε και τα δεδομένα αποθηκεύτηκαν στο '{output_path}'\")\n",
    "\n",
    "# Αποθήκευση του ευρετηρίου (Inverted Index)\n",
    "index_output_path = \"inverted_index.json\"\n",
    "with open(index_output_path, \"w\", encoding=\"utf-8\") as index_file:\n",
    "    json.dump(inverted_index, index_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Το ευρετήριο αποθηκεύτηκε στο '{index_output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βήμα 4. Μηχανή αναζήτησης (Search Engine): \n",
    "Ανάπτυξη διεπαφής χρήστη για την αναζήτηση όρων χρησιμοποιώντας την Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α) Επεξεργασία ερωτήματος (Query Processing):\n",
    "Ανάπτυξη ενός module επεξεργασίας ερωτημάτων που θα προεπεξεργάζεται τα ερωτήματα που λαμβάνει από τον χρήστη, τα αναλύει και ανακτά σχετικά έγγραφα χρησιμοποιώντας το ανεστραμμένο ευρετήριο. Οι χρήστες θα μπορούν να αναζητούν έγγραφα χρησιμοποιώντας μία ή περισσότερες λέξεις. Το module θα λαμβάνει ερωτήματα χρηστών τα οποία τα γίνονται tokenized και θα εκτελεί απλές λειτουργίες Boolean (AND, OR και NOT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Καλωσορίσατε στη Μηχανή Αναζήτησης!\n",
      "\n",
      "Μπορείτε να χρησιμοποιήσετε τους παρακάτω τελεστές για τη σύνθεση των ερωτημάτων σας:\n",
      "- OR: Επιστρέφει έγγραφα που περιέχουν οποιονδήποτε από τους όρους (π.χ. 'machine OR learning').\n",
      "- AND: Επιστρέφει έγγραφα που περιέχουν όλους τους όρους (π.χ. 'machine AND learning').\n",
      "- NOT: Εξαιρεί έγγραφα που περιέχουν τον όρο (π.χ. 'NOT neural').\n",
      "\n",
      "Εισάγετε το ερώτημά σας για αναζήτηση:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Βρέθηκαν 1 σχετικά έγγραφα: [1]\n",
      "\n",
      "Εισάγετε το ερώτημά σας για αναζήτηση:\n",
      "Αντίο! Σας ευχαριστούμε που χρησιμοποιήσατε τη Μηχανή Αναζήτησης.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Φόρτωση του ανεστραμμένου ευρετηρίου\n",
    "with open(\"inverted_index.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    inverted_index = json.load(file)\n",
    "\n",
    "# Συνάρτηση για την ανάκτηση εγγράφων βάσει όρου\n",
    "def get_documents(term):\n",
    "    return set(inverted_index.get(term, []))\n",
    "\n",
    "# Συνάρτηση για την επεξεργασία ερωτήματος\n",
    "def process_query(query):\n",
    "    # Διάσπαση του ερωτήματος σε tokens\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Στοίβα για διαχείριση Boolean τελεστών\n",
    "    stack = []\n",
    "\n",
    "    # Διαχείριση Boolean τελεστών\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i].lower()\n",
    "\n",
    "        if token == \"and\":\n",
    "            # Λειτουργία AND\n",
    "            operand1 = stack.pop()\n",
    "            operand2 = get_documents(tokens[i + 1])\n",
    "            stack.append(operand1 & operand2)\n",
    "            i += 1\n",
    "        elif token == \"or\":\n",
    "            # Λειτουργία OR\n",
    "            operand1 = stack.pop()\n",
    "            operand2 = get_documents(tokens[i + 1])\n",
    "            stack.append(operand1 | operand2)\n",
    "            i += 1\n",
    "        elif token == \"not\":\n",
    "            # Λειτουργία NOT\n",
    "            operand2 = get_documents(tokens[i + 1])\n",
    "            stack.append(set(range(len(inverted_index))) - operand2)\n",
    "            i += 1\n",
    "        else:\n",
    "            # Επεξεργασία όρου\n",
    "            stack.append(get_documents(token))\n",
    "        i += 1\n",
    "\n",
    "    # Το τελευταίο στοιχείο στη στοίβα είναι το αποτέλεσμα\n",
    "    return stack.pop()\n",
    "\n",
    "# Διεπαφή γραμμής εντολών για τον χρήστη\n",
    "print(\"Καλωσορίσατε στη Μηχανή Αναζήτησης!\")\n",
    "print(\"\\nΜπορείτε να χρησιμοποιήσετε τους παρακάτω τελεστές για τη σύνθεση των ερωτημάτων σας:\")\n",
    "print(\"- OR: Επιστρέφει έγγραφα που περιέχουν οποιονδήποτε από τους όρους (π.χ. 'machine OR learning').\")\n",
    "print(\"- AND: Επιστρέφει έγγραφα που περιέχουν όλους τους όρους (π.χ. 'machine AND learning').\")\n",
    "print(\"- NOT: Εξαιρεί έγγραφα που περιέχουν τον όρο (π.χ. 'NOT neural').\")\n",
    "\n",
    "while True:\n",
    "    print(\"\\nΕισάγετε το ερώτημά σας για αναζήτηση:\")\n",
    "    query = input(\"Πληκτρολογήστε 'q' για έξοδο.\\n> \").strip()\n",
    "    if query.lower() == \"q\":\n",
    "        print(\"Αντίο! Σας ευχαριστούμε που χρησιμοποιήσατε τη Μηχανή Αναζήτησης.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Επεξεργασία του ερωτήματος\n",
    "        result = process_query(query)\n",
    "\n",
    "        # Εμφάνιση αποτελεσμάτων\n",
    "        if result:\n",
    "            print(f\"\\nΒρέθηκαν {len(result)} σχετικά έγγραφα: {sorted(result)}\")\n",
    "        else:\n",
    "            print(\"\\nΔεν βρέθηκαν σχετικά έγγραφα.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nΣφάλμα κατά την επεξεργασία του ερωτήματος: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Κατάταξη αποτελεσμάτων (Ranking): \n",
    "Εφαρμογή αλγορίθμων κατάταξης:\n",
    "Boolean Retrieval, TF-IDF Ranking, Okapi BM25\n",
    "\n",
    "Ο χρήστης μπορεί να επιλέγει τον αλγόριθμο ανάκτησης.\n",
    "Ταξινόμηση και παρουσίαση των αποτελεσμάτων αναζήτησης σε φιλική προς το χρήστη μορφή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Καλωσορίσατε στη Μηχανή Αναζήτησης!\n",
      "\n",
      "Μπορείτε να χρησιμοποιήσετε τους παρακάτω τελεστές για τη σύνθεση των ερωτημάτων σας:\n",
      "- OR: Επιστρέφει έγγραφα που περιέχουν οποιονδήποτε από τους όρους (π.χ. 'machine OR learning').\n",
      "- AND: Επιστρέφει έγγραφα που περιέχουν όλους τους όρους (π.χ. 'machine AND learning').\n",
      "- NOT: Εξαιρεί έγγραφα που περιέχουν τον όρο (π.χ. 'NOT neural').\n",
      "\n",
      "Επιλέξτε τον αλγόριθμο κατάταξης:\n",
      "1: Boolean Retrieval\n",
      "2: TF-IDF Ranking\n",
      "3: Okapi BM25\n",
      "Πληκτρολογήστε 'q' για έξοδο.\n",
      "\n",
      "Εισάγετε το ερώτημά σας για αναζήτηση:\n",
      "\n",
      "Σφάλμα κατά την επεξεργασία του ερωτήματος: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Επιλέξτε τον αλγόριθμο κατάταξης:\n",
      "1: Boolean Retrieval\n",
      "2: TF-IDF Ranking\n",
      "3: Okapi BM25\n",
      "Πληκτρολογήστε 'q' για έξοδο.\n",
      "Μη έγκυρη επιλογή, παρακαλώ προσπαθήστε ξανά.\n",
      "\n",
      "Επιλέξτε τον αλγόριθμο κατάταξης:\n",
      "1: Boolean Retrieval\n",
      "2: TF-IDF Ranking\n",
      "3: Okapi BM25\n",
      "Πληκτρολογήστε 'q' για έξοδο.\n",
      "Μη έγκυρη επιλογή, παρακαλώ προσπαθήστε ξανά.\n",
      "\n",
      "Επιλέξτε τον αλγόριθμο κατάταξης:\n",
      "1: Boolean Retrieval\n",
      "2: TF-IDF Ranking\n",
      "3: Okapi BM25\n",
      "Πληκτρολογήστε 'q' για έξοδο.\n",
      "Μη έγκυρη επιλογή, παρακαλώ προσπαθήστε ξανά.\n",
      "\n",
      "Επιλέξτε τον αλγόριθμο κατάταξης:\n",
      "1: Boolean Retrieval\n",
      "2: TF-IDF Ranking\n",
      "3: Okapi BM25\n",
      "Πληκτρολογήστε 'q' για έξοδο.\n",
      "Αντίο! Σας ευχαριστούμε που χρησιμοποιήσατε τη Μηχανή Αναζήτησης.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Φόρτωση του ανεστραμμένου ευρετηρίου\n",
    "with open(\"inverted_index.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    inverted_index = json.load(file)\n",
    "\n",
    "# Συνάρτηση για την ανάκτηση εγγράφων βάσει όρου\n",
    "def get_documents(term):\n",
    "    return set(inverted_index.get(term, []))\n",
    "\n",
    "# Συνάρτηση για την επεξεργασία ερωτήματος\n",
    "def process_query(query):\n",
    "    tokens = query.split()\n",
    "    stack = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i].lower()\n",
    "        if token == \"and\":\n",
    "            operand1 = stack.pop()\n",
    "            operand2 = get_documents(tokens[i + 1])\n",
    "            stack.append(operand1 & operand2)\n",
    "            i += 1\n",
    "        elif token == \"or\":\n",
    "            operand1 = stack.pop()\n",
    "            operand2 = get_documents(tokens[i + 1])\n",
    "            stack.append(operand1 | operand2)\n",
    "            i += 1\n",
    "        elif token == \"not\":\n",
    "            operand2 = get_documents(tokens[i + 1])\n",
    "            stack.append(set(range(len(inverted_index))) - operand2)\n",
    "            i += 1\n",
    "        else:\n",
    "            stack.append(get_documents(token))\n",
    "        i += 1\n",
    "    return stack.pop()\n",
    "\n",
    "# Συνάρτηση TF-IDF για την κατάταξη αποτελεσμάτων\n",
    "def compute_tfidf(query, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    similarities = np.dot(query_vector, tfidf_matrix.T).toarray().flatten()\n",
    "    return similarities\n",
    "\n",
    "# Συνάρτηση για τον υπολογισμό της BM25\n",
    "def compute_bm25(query, documents, k1=1.5, b=0.75):\n",
    "    avg_doc_len = np.mean([len(doc.split()) for doc in documents])\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    doc_len = [len(doc.split()) for doc in documents]\n",
    "    \n",
    "    for doc in documents:\n",
    "        for term in doc.split():\n",
    "            idf[term] += 1\n",
    "    \n",
    "    idf = {term: math.log((len(documents) - freq + 0.5) / (freq + 0.5) + 1.0) for term, freq in idf.items()}\n",
    "    \n",
    "    scores = []\n",
    "    for doc, length in zip(documents, doc_len):\n",
    "        score = 0\n",
    "        for term in query.split():\n",
    "            term_freq = doc.split().count(term)\n",
    "            score += idf.get(term, 0) * (term_freq * (k1 + 1)) / (term_freq + k1 * (1 - b + b * length / avg_doc_len))\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Συνάρτηση για την ταξινόμηση αποτελεσμάτων\n",
    "def rank_documents(query, documents, ranking_algorithm=\"boolean\"):\n",
    "    if ranking_algorithm == \"boolean\":\n",
    "        # Αναζήτηση με Boolean\n",
    "        result = process_query(query)\n",
    "        return sorted(result)\n",
    "    elif ranking_algorithm == \"tfidf\":\n",
    "        # TF-IDF Ranking\n",
    "        similarities = compute_tfidf(query, documents)\n",
    "        ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "        return [doc[0] for doc in ranked_docs]\n",
    "    elif ranking_algorithm == \"bm25\":\n",
    "        # BM25 Ranking\n",
    "        scores = compute_bm25(query, documents)\n",
    "        ranked_docs = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return [doc[0] for doc in ranked_docs]\n",
    "\n",
    "# Διεπαφή γραμμής εντολών για τον χρήστη\n",
    "def main():\n",
    "    print(\"Καλωσορίσατε στη Μηχανή Αναζήτησης!\")\n",
    "    print(\"\\nΜπορείτε να χρησιμοποιήσετε τους παρακάτω τελεστές για τη σύνθεση των ερωτημάτων σας:\")\n",
    "    print(\"- OR: Επιστρέφει έγγραφα που περιέχουν οποιονδήποτε από τους όρους (π.χ. 'machine OR learning').\")\n",
    "    print(\"- AND: Επιστρέφει έγγραφα που περιέχουν όλους τους όρους (π.χ. 'machine AND learning').\")\n",
    "    print(\"- NOT: Εξαιρεί έγγραφα που περιέχουν τον όρο (π.χ. 'NOT neural').\")\n",
    "    \n",
    "    documents = [\"\"]  # Λίστα για τα έγγραφα (π.χ. κάθε έγγραφο ως απλό κείμενο)\n",
    "    while True:\n",
    "        print(\"\\nΕπιλέξτε τον αλγόριθμο κατάταξης:\")\n",
    "        print(\"1: Boolean Retrieval\")\n",
    "        print(\"2: TF-IDF Ranking\")\n",
    "        print(\"3: Okapi BM25\")\n",
    "        print(\"Πληκτρολογήστε 'q' για έξοδο.\")\n",
    "        \n",
    "        choice = input(\"Επιλογή αλγορίθμου (1/2/3): \").strip()\n",
    "        if choice.lower() == \"q\":\n",
    "            print(\"Αντίο! Σας ευχαριστούμε που χρησιμοποιήσατε τη Μηχανή Αναζήτησης.\")\n",
    "            break\n",
    "\n",
    "        # Επιλογή αλγορίθμου κατάταξης\n",
    "        if choice == \"1\":\n",
    "            ranking_algorithm = \"boolean\"\n",
    "        elif choice == \"2\":\n",
    "            ranking_algorithm = \"tfidf\"\n",
    "        elif choice == \"3\":\n",
    "            ranking_algorithm = \"bm25\"\n",
    "        else:\n",
    "            print(\"Μη έγκυρη επιλογή, παρακαλώ προσπαθήστε ξανά.\")\n",
    "            continue\n",
    "\n",
    "        # Εισαγωγή ερωτήματος\n",
    "        print(\"\\nΕισάγετε το ερώτημά σας για αναζήτηση:\")\n",
    "        query = input(\"Πληκτρολογήστε 'q' για έξοδο.\\n> \").strip()\n",
    "        if query.lower() == \"q\":\n",
    "            print(\"Αντίο! Σας ευχαριστούμε που χρησιμοποιήσατε τη Μηχανή Αναζήτησης.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Κατάταξη και εμφάνιση αποτελεσμάτων\n",
    "            ranked_docs = rank_documents(query, documents, ranking_algorithm)\n",
    "            if ranked_docs:\n",
    "                print(f\"\\nΒρέθηκαν {len(ranked_docs)} σχετικά έγγραφα: {ranked_docs}\")\n",
    "            else:\n",
    "                print(\"\\nΔεν βρέθηκαν σχετικά έγγραφα.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nΣφάλμα κατά την επεξεργασία του ερωτήματος: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
